{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/data_investeringer.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "df = df[df['ISIN kode'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '-' with NaN (remove entries with no value)\n",
    "df['Markedsværdi (DKK)'] = df['Markedsværdi (DKK)'].replace('-', np.nan)\n",
    "\n",
    "# Remove any non-numeric characters except for digits and decimal points\n",
    "df['Markedsværdi (DKK)'] = df['Markedsværdi (DKK)'].replace(r'[^\\d.]', '', regex=True)\n",
    "\n",
    "# Convert the column to float (after cleaning)\n",
    "df['Markedsværdi (DKK)'] = pd.to_numeric(df['Markedsværdi (DKK)'], errors='coerce')\n",
    "\n",
    "# Display the updated dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/pfa_eksklusionsliste.xlsx\"\n",
    "pfa = pd.read_excel(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfa = pfa[pfa['Land'].notna()]\n",
    "pfa['Kilde til liste'] = 'PFA'\n",
    "pfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tilføj ISIN-numre til eksklusionsliste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data\n",
    "df1 = df\n",
    "df2 = pfa\n",
    "\n",
    "# Normalize company names by converting to lowercase and removing special characters\n",
    "def normalize_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    # Lowercase and remove special characters\n",
    "    return re.sub(r'\\W+', '', name.lower())\n",
    "\n",
    "# Apply normalization to relevant columns\n",
    "df1['Udsteder_normalized'] = df1['Udsteder'].apply(normalize_name)\n",
    "df1['Værdipapirets navn_normalized'] = df1['Værdipapirets navn'].apply(normalize_name)\n",
    "df2['Selskab_normalized'] = df2['Selskab'].apply(normalize_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find ISINs, Udsteder, and Værdipapirets navn based on partial match\n",
    "def find_isin_and_names(selskab, df1):\n",
    "    matches = df1[\n",
    "        (df1['Udsteder_normalized'].str.contains(selskab)) |\n",
    "        (df1['Værdipapirets navn_normalized'].str.contains(selskab))\n",
    "    ]\n",
    "    # Return unique ISINs, Udsteder, and Værdipapirets navn if matches are found\n",
    "    if not matches.empty:\n",
    "        isins = matches['ISIN kode'].unique().tolist()\n",
    "        udsteder = matches['Udsteder'].unique().tolist()\n",
    "        værdipapirets_navn = matches['Værdipapirets navn'].unique().tolist()\n",
    "        return isins, udsteder, værdipapirets_navn\n",
    "    else:\n",
    "        return [], [], []\n",
    "\n",
    "# Apply the function to each row in df2 and create new columns for ISIN, Udsteder, and Værdipapirets navn\n",
    "df2[['ISIN', 'Matched Udsteder', 'Matched Værdipapirets navn']] = df2['Selskab_normalized'].apply(\n",
    "    lambda x: pd.Series(find_isin_and_names(x, df1))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df2 as an Excel file\n",
    "# file_path = \"../data/pfa_eksklutionsliste_isin.xlsx\"\n",
    "# df2.to_excel(file_path, index=False)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Updated Second DataFrame with ISIN, Udsteder, and Værdipapirets navn\", dataframe=df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tilføj kolonner til oprindelig liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume df1 and df2 are already defined\n",
    "\n",
    "# Initialize new columns in df1 with empty strings\n",
    "df1['Problematisk ifølge:'] = \"\"\n",
    "df1['Forklaring'] = \"\"\n",
    "\n",
    "# Iterate over each row in df2\n",
    "for index, row in df2.iterrows():\n",
    "    # Get the list of ISINs from df2\n",
    "    isin_list = row['ISIN']\n",
    "    kilde = row['Kilde til liste']\n",
    "    forklaring = row['Årsag til eksklusion']\n",
    "    \n",
    "    # Find matching ISINs in df1\n",
    "    df1_matches = df1[df1['ISIN kode'].isin(isin_list)]\n",
    "    \n",
    "    # Update df1 with the matching values from df2\n",
    "    df1.loc[df1_matches.index, 'Problematisk ifølge:'] = kilde\n",
    "    df1.loc[df1_matches.index, 'Forklaring'] = forklaring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df1 to show only rows where there is a match\n",
    "matched_rows = df1[df1['Problematisk ifølge:'] != \"\"]\n",
    "\n",
    "matched_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL of the exclusion list\n",
    "url = \"https://akademikerpension.dk/ansvarlighed/frasalg-og-eksklusion/\"\n",
    "\n",
    "# Send a request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Define exclusion categories and their reasons\n",
    "exclusion_categories = {\n",
    "    'Kul': 'Kul',\n",
    "    'Menneskerettigheder': 'Menneskerettigheder',\n",
    "    'Olie og gas': 'Olie og gas',\n",
    "    'Tobak': 'Tobak',\n",
    "    'Tjæresand': 'Tjæresand',\n",
    "    'Våben': 'Våben'\n",
    "}\n",
    "\n",
    "# Initialize empty lists to store the data\n",
    "companies = []\n",
    "reasons = []\n",
    "\n",
    "# Loop through each exclusion category\n",
    "for category, reason in exclusion_categories.items():\n",
    "    # Find the section with the category title\n",
    "    category_section = soup.find('h2', class_='card__title', text=lambda x: x and category in x)\n",
    "    \n",
    "    if category_section:\n",
    "        # Find the list of companies in the next \"ul\" after the category\n",
    "        company_list = category_section.find_next('ul')\n",
    "        \n",
    "        if company_list:\n",
    "            for company in company_list.find_all('p', class_='navigation-item__link-title'):\n",
    "                companies.append(company.text.strip())\n",
    "                reasons.append(reason)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "df = pd.DataFrame({\n",
    "    'Company Name': companies,\n",
    "    'Reason': reasons\n",
    "})\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "file_path = 'akademikerpension_exclusions.xlsx'\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "file_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
